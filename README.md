# ğŸ§  LLM-API Flask Server

A **production-ready API server** built using **Flask**, **Hugging Face Transformers**, and **PyTorch** â€” designed to deploy a **local Large Language Model (LLM)** with support for both **GPU (CUDA)** and **CPU** environments.  

The project comes fully **Dockerized** for easy setup, testing, and deployment in any environment.

---

## ğŸš€ Features

- âš¡ REST API endpoints for LLM-based text generation  
- ğŸ§© GPU (CUDA) acceleration with automatic fallback to CPU  
- ğŸ³ Docker & Docker Compose support for quick deployment  
- ğŸ§± Modular structure â€” easy to extend, customize, and scale  
- ğŸ’¾ Persistent model caching for faster subsequent runs  

---

## ğŸ“‚ Project Structure

```

.
â”œâ”€â”€ api.py              # Flask API for HTTP endpoints
â”œâ”€â”€ llm.py              # Model loading and inference logic
â”œâ”€â”€ requirements.txt    # Python and library dependencies
â”œâ”€â”€ Dockerfile          # Docker image definition
â”œâ”€â”€ docker-compose.yml  # Multi-container setup with GPU support
â””â”€â”€ README.md           # Project documentation

````

---

## âš™ï¸ Installation

### ğŸ§© Prerequisites

- [Docker](https://www.docker.com/)  
- (Optional) **NVIDIA GPU drivers** and **Dockerâ€™s NVIDIA runtime** for GPU acceleration  

---

### ğŸ Setup

Clone the repository:

```bash
git clone https://github.com/rajvansh-369/llm_chatbot_api.git
cd llm_chatbot_api
````

(Optional) Edit `llm.py` or `api.py` to change model settings or API routes.

---

## ğŸ§± Running the API (Docker Compose)

Build and start the containers:

```bash
docker compose up --build
```

Once running, your API will be accessible at:

ğŸ‘‰ **[http://localhost:5000](http://localhost:5000)**

---

## ğŸŒ API Endpoints

### 1. **Greet Endpoint**

**Request:**

```bash
GET /greet?name=John
```

**Response:**

```html
<h1>Hello, John!</h1>
```

---

### 2. **LLM Input Endpoint**

**Request:**

```bash
POST /input
Content-Type: application/json

{
  "prompt": "Who won the FIFA World Cup in 2022?"
}
```

**Response:**

```json
{
  "prompt": "Who won the FIFA World Cup in 2022?",
  "response": "<model's generated answer>",
  "status": "success"
}
```

---

## ğŸ§  Development

* Modify `llm.py` to switch models or adjust generation parameters.
* Edit `api.py` to add or modify REST API endpoints.
* The first run may take longer as the model downloads and caches locally.

ğŸ—‚ Cached model weights are stored in the `./cache` directory (mapped in `docker-compose.yml`).

---

## âš¡ GPU Support

For GPU acceleration:

* Ensure **CUDA** is installed and configured on your system.
* Use Docker with **NVIDIA runtime**.
* Confirm GPU visibility inside the container using:

```bash
docker compose exec <service_name> nvidia-smi
```

---

## ğŸ§¾ Example Output

```bash
curl -X POST http://localhost:5000/input \
-H "Content-Type: application/json" \
-d '{"prompt": "Explain quantum computing in simple terms."}'
```

**Response:**

```json
{
  "prompt": "Explain quantum computing in simple terms.",
  "response": "Quantum computing uses qubits, which can represent 0 and 1 simultaneously...",
  "status": "success"
}
```

---

## ğŸ§© Tech Stack

* [Flask](https://flask.palletsprojects.com/) â€“ Lightweight Python web framework
* [Transformers](https://huggingface.co/transformers/) â€“ State-of-the-art NLP models
* [PyTorch](https://pytorch.org/) â€“ Deep learning framework
* [Docker](https://www.docker.com/) â€“ Containerized deployment

---

## ğŸ—ï¸ Future Improvements

* âœ… Add authentication (API key or JWT)
* âœ… Add streaming responses for long text generation
* âœ… Add async inference for faster parallel requests
* âœ… Add WebSocket endpoint for real-time interaction

---

## ğŸ™Œ Acknowledgments

* [Hugging Face Transformers](https://huggingface.co/transformers/)
* [PyTorch](https://pytorch.org/)
* [Flask](https://flask.palletsprojects.com/)
* [Docker](https://www.docker.com/)

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” feel free to use and modify it for your own projects.

---

### ğŸ‘¤ Author

**Snehal Rajvansh**
ğŸ”— [GitHub](https://github.com/rajvansh-369)
ğŸ“§ [Contact](mailto:rajbansh.snehal@gmail.com)


